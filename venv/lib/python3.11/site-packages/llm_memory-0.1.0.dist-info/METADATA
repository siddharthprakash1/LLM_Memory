Metadata-Version: 2.4
Name: llm-memory
Version: 0.1.0
Summary: Hierarchical Long-Term Memory for LLM Agents - Real memory with decay, consolidation, and intent-aware retrieval
Project-URL: Homepage, https://github.com/llm-memory/llm-memory
Project-URL: Documentation, https://github.com/llm-memory/llm-memory#readme
Project-URL: Repository, https://github.com/llm-memory/llm-memory
Project-URL: Issues, https://github.com/llm-memory/llm-memory/issues
Author: LLM Memory Project
License-Expression: MIT
Keywords: agents,ai,cognitive-architecture,embeddings,llm,memory,vector-database
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.11
Requires-Dist: aiosqlite>=0.19.0
Requires-Dist: chromadb>=0.4.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: openai>=1.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-ulid>=2.0.0
Provides-Extra: agent
Requires-Dist: langchain-core>=0.2.0; extra == 'agent'
Requires-Dist: langchain-ollama>=0.1.0; extra == 'agent'
Requires-Dist: langchain>=0.2.0; extra == 'agent'
Requires-Dist: langgraph>=0.2.0; extra == 'agent'
Provides-Extra: dev
Requires-Dist: black>=23.0.0; extra == 'dev'
Requires-Dist: hypothesis>=6.0.0; extra == 'dev'
Requires-Dist: mypy>=1.0.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.0.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Provides-Extra: langchain
Requires-Dist: langchain-core>=0.1.0; extra == 'langchain'
Requires-Dist: langchain>=0.1.0; extra == 'langchain'
Description-Content-Type: text/markdown

# LLM Memory - Hierarchical Long-Term Memory for AI Agents

A comprehensive, production-ready memory system for AI agents implementing cognitive architecture principles with three-tier hierarchical memory, decay functions, conflict resolution, and intent-aware retrieval.

## ğŸŒŸ Features

### Memory Hierarchy
- **Short-Term Memory (STM)**: Conversation context and working memory with attention-based salience
- **Episodic Memory**: Event-based memories with temporal context and emotional tagging
- **Semantic Memory**: Factual knowledge with structured fact storage and generalization

### Memory Decay
- **Ebbinghaus Forgetting Curve**: Realistic memory decay over time
- **Importance Scoring**: Multi-factor importance calculation (emotional salience, novelty, relevance)
- **Rehearsal Boost**: Memory strengthening through repeated access

### Consolidation Pipeline
- **STM â†’ Episodic Promotion**: Automatic promotion based on importance and age
- **Episodic â†’ Semantic Extraction**: Fact extraction and knowledge generalization
- **Memory Merging**: Intelligent deduplication and fact strengthening
- **Garbage Collection**: Automatic cleanup of decayed memories

### Conflict Resolution
- **Contradiction Detection**: Identifies temporal, factual, and preference conflicts
- **Multiple Strategies**: Recency, confidence, source reliability, importance-based resolution
- **User-Guided Resolution**: Support for human-in-the-loop conflict resolution

### Intent-Aware Retrieval
- **Query Intent Classification**: Factual, procedural, episodic, preference, problem-solving
- **Multi-Tier Search**: Searches across all memory tiers with intent weighting
- **Result Re-ranking**: Importance, recency, and relevance-based ranking

### Integrations
- **LangChain Compatible**: Drop-in memory class for LangChain chains
- **Event Hooks**: Extensible pub/sub system for memory lifecycle events
- **RAG Support**: Memory retriever for retrieval-augmented generation

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-memory.git
cd llm-memory

# Create virtual environment (Python 3.11+ recommended)
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -e ".[dev]"
```

## ğŸš€ Quick Start

```python
import asyncio
from llm_memory import MemorySystem, MemoryType

async def main():
    # Initialize the memory system
    system = MemorySystem()
    await system.initialize()
    
    # Store a semantic memory (fact)
    await system.remember(
        "User prefers Python for data science",
        memory_type=MemoryType.SEMANTIC,
        tags=["preference", "language"]
    )
    
    # Store an episodic memory (experience)
    await system.remember(
        "Debugged async database issue - solution was missing await",
        memory_type=MemoryType.EPISODIC,
        tags=["debugging", "async"]
    )
    
    # Add conversation messages
    await system.add_message("Hello!", role="user", session_id="chat_1")
    await system.add_message("Hi there!", role="assistant", session_id="chat_1")
    
    # Recall relevant memories
    results = await system.recall("What language does user prefer?")
    for result in results.ranked_results:
        print(f"- {result.result.content}")
    
    # Get conversation context with relevant memories
    context = await system.get_context(
        session_id="chat_1",
        include_relevant=True,
        query="programming"
    )
    
    await system.close()

asyncio.run(main())
```

## ğŸ”§ Configuration

```python
from llm_memory import MemoryConfig, MemorySystem, MemorySystemConfig

# Main configuration
config = MemoryConfig(
    decay={"half_life_hours": 24, "min_strength": 0.1},
    consolidation={"stm_promotion_age_minutes": 30},
    embedding={"provider": "ollama", "model": "nomic-embed-text"},
    llm={"provider": "ollama", "model": "llama3.2"},
)

# System configuration
system_config = MemorySystemConfig(
    enable_embeddings=True,
    enable_summarization=True,
    enable_consolidation=True,
    enable_conflict_resolution=True,
    auto_consolidate=True,
    consolidation_interval_seconds=300,
)

system = MemorySystem(config=config, system_config=system_config)
```

## ğŸ”— LangChain Integration

```python
from llm_memory.api.integrations.langchain import LangChainMemory, HierarchicalMemory

# Basic chat memory (drop-in replacement)
memory = LangChainMemory(session_id="my_session")

# With LangChain ConversationChain
from langchain.chains import ConversationChain
chain = ConversationChain(llm=llm, memory=memory)

# Hierarchical memory with long-term context
memory = HierarchicalMemory(
    include_semantic=True,
    include_episodic=True,
    max_context_memories=5,
)
```

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Memory System                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   STM     â”‚  â”‚ Episodic  â”‚  â”‚ Semantic  â”‚                â”‚
â”‚  â”‚ (Buffer)  â”‚â†’â†’â”‚ (Events)  â”‚â†’â†’â”‚ (Facts)   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚        â†“              â†“              â†“                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚         Consolidation Pipeline          â”‚                â”‚
â”‚  â”‚  â€¢ Promotion  â€¢ Merging  â€¢ GC           â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚        â†“              â†“              â†“                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚          Conflict Resolution            â”‚                â”‚
â”‚  â”‚  â€¢ Detection  â€¢ Strategies  â€¢ Merging   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚        â†“              â†“              â†“                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚        Intent-Aware Retrieval           â”‚                â”‚
â”‚  â”‚  â€¢ Classification  â€¢ Search  â€¢ Ranking  â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest -v

# Run with coverage
pytest --cov=llm_memory --cov-report=html

# Run specific test modules
pytest tests/test_models.py -v
pytest tests/test_consolidation.py -v
pytest tests/test_conflict.py -v
```

## ğŸ“ Project Structure

```
llm_memory/
â”œâ”€â”€ models/           # Data models (STM, Episodic, Semantic)
â”œâ”€â”€ encoding/         # Embedder and Summarizer implementations
â”œâ”€â”€ decay/            # Decay functions and importance scoring
â”œâ”€â”€ consolidation/    # Promotion, merging, garbage collection
â”œâ”€â”€ conflict/         # Detection, strategies, resolution
â”œâ”€â”€ retrieval/        # Intent classification, search, ranking
â”œâ”€â”€ storage/          # SQLite and Vector storage backends
â””â”€â”€ api/              # MemorySystem, MemoryAPI, hooks, integrations

tests/                # Comprehensive test suite
examples/             # Usage examples
```

## ğŸ¤– LangGraph Agent

The project includes a ready-to-use LangGraph-powered agent with memory:

```python
import asyncio
from llm_memory.agent import MemoryAgent, AgentConfig

async def main():
    # Create agent with Ollama
    config = AgentConfig(ollama_model="llama3.2")
    agent = MemoryAgent(config)
    await agent.initialize()
    
    # Chat with memory
    response = await agent.chat("My name is Alex and I love Python!")
    print(response)
    
    response = await agent.chat("What's my name?")  # Recalls: Alex
    print(response)
    
    await agent.close()

asyncio.run(main())
```

### Interactive CLI

```bash
# Run the memory agent CLI
memory-agent --model llama3.2

# Or with Python
python -m llm_memory.agent.cli --model llama3.2
```

CLI Commands:
- `/memory` - Show memory statistics
- `/remember <text>` - Manually store a memory
- `/recall <query>` - Search memories
- `/help` - Show all commands

## ğŸ¯ Use Cases

- **AI Assistants**: Long-term memory for personalized interactions
- **Copilot Systems**: Remember user preferences and project context
- **Conversational AI**: Context-aware responses with memory recall
- **Knowledge Management**: Structured fact storage and retrieval
- **RAG Systems**: Memory-augmented retrieval for LLMs

## ğŸ“ˆ Performance

- 270+ passing tests
- ~76% code coverage
- Async-first design for high concurrency
- In-memory caching with optional persistence
- Efficient batch operations

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Inspired by cognitive architecture research
- Built with [Pydantic](https://pydantic.dev/) for data validation
- Vector storage powered by [ChromaDB](https://www.trychroma.com/)
- Local LLM support via [Ollama](https://ollama.ai/)
