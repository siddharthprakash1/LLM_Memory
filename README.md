# ğŸ§  LLM Memory

**Hierarchical Long-Term Memory for LLM Agents**

> Real memory, not chat history. A cognitive architecture with decay, consolidation, and intent-aware retrieval.

---

## ğŸ¯ The Problem

Most "memory" systems for LLMs are just vector similarity search over conversation history. They lack:
- **Memory decay** - everything stays equally "fresh" forever
- **Consolidation** - experiences never become knowledge
- **Conflict resolution** - contradictory facts coexist silently
- **Intent-aware retrieval** - all queries are treated the same

## ğŸ’¡ The Solution

A three-tier hierarchical memory system inspired by cognitive science:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RETRIEVAL LAYER                              â”‚
â”‚         (Intent-aware, task-scoped memory access)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHORT-TERM     â”‚ â”‚   EPISODIC      â”‚ â”‚   SEMANTIC      â”‚
â”‚  MEMORY (STM)   â”‚ â”‚   MEMORY        â”‚ â”‚   MEMORY        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Working ctx   â”‚ â”‚ â€¢ Events        â”‚ â”‚ â€¢ Facts         â”‚
â”‚ â€¢ Current task  â”‚ â”‚ â€¢ Experiences   â”‚ â”‚ â€¢ Patterns      â”‚
â”‚ â€¢ Fast decay    â”‚ â”‚ â€¢ Temporal tags â”‚ â”‚ â€¢ Generalizationsâ”‚
â”‚ â€¢ High capacity â”‚ â”‚ â€¢ Medium decay  â”‚ â”‚ â€¢ Slow decay    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â–º CONSOLIDATION â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   (STM â†’ Episodic â†’ Semantic)
```

## âœ¨ Features

- **Three-tier memory hierarchy**: Short-term â†’ Episodic â†’ Semantic
- **Memory decay**: Ebbinghaus forgetting curve with importance weighting
- **Automatic consolidation**: Memories are promoted and abstracted over time
- **Conflict detection & resolution**: Handle contradictory information gracefully
- **Intent-aware retrieval**: Different query types access different memory strategies
- **Scoped contexts**: Project, user, and global memory scopes

## ğŸ“¦ Installation

```bash
pip install llm-memory
```

Or from source:

```bash
git clone https://github.com/llm-memory/llm-memory.git
cd llm-memory
pip install -e ".[dev]"
```

## ğŸš€ Quick Start

```python
from llm_memory import MemorySystem

# Initialize memory system
memory = MemorySystem(
    user_id="user_123",
    scope="project_abc"
)

# Observe conversations (auto-extracts and stores memories)
memory.observe("I prefer using Python for backend development", role="user")
memory.observe("I'll use Python for this project then", role="assistant")

# Recall relevant memories based on intent
context = memory.recall(
    query="What language should I use for the API?",
    intent="preference",
    limit=5
)

# Memory reflection (agent reviews its memories)
insights = memory.reflect(topic="user preferences")
```

## ğŸ—ï¸ Architecture

### Memory Types

| Type | Purpose | Decay Rate | Example |
|------|---------|------------|---------|
| **Short-term** | Working context | Fast (minutes) | Current conversation buffer |
| **Episodic** | Event memories | Medium (days) | "User debugged auth issue on Monday" |
| **Semantic** | Facts & patterns | Slow (weeks) | "User prefers async Python" |

### Importance Scoring

Memories are scored for importance based on:
- **Emotional salience** (sentiment analysis)
- **Novelty** (how different from existing memories)
- **Relevance frequency** (how often retrieved)
- **Causal significance** (affects downstream events)
- **User feedback** (explicit importance markers)

### Consolidation Pipeline

```
STM â†’ Episodic:
  Trigger: End of task/session OR importance threshold
  Transform: Raw context â†’ Structured episode with temporal tags
  
Episodic â†’ Semantic:
  Trigger: N similar episodes detected
  Transform: Specific events â†’ General pattern/fact
```

## ğŸ“– Documentation

- [Architecture Deep Dive](docs/architecture.md)
- [Cognitive Foundations](docs/cognitive_foundations.md)
- [API Reference](docs/api_reference.md)

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=llm_memory --cov-report=html

# Run specific test file
pytest tests/test_models.py -v
```

## ğŸ—ºï¸ Roadmap

- [x] Stage 1: Foundation & Memory Store
- [ ] Stage 2: Memory Encoding & Decay
- [ ] Stage 3: Consolidation Pipeline
- [ ] Stage 4: Conflict Resolution
- [ ] Stage 5: Intent-Aware Retrieval
- [ ] Stage 6: Agent Integration
- [ ] Stage 7: Evaluation & Tuning

## ğŸ¤ Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests.

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

**Built with ğŸ§  for smarter AI agents**
